{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":9762249,"sourceType":"datasetVersion","datasetId":5978239},{"sourceId":9743038,"sourceType":"datasetVersion","datasetId":5964042}],"dockerImageVersionId":30786,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom sklearn.preprocessing import LabelEncoder, StandardScaler\nfrom sklearn.metrics import mean_squared_error\nimport pandas as pd\nimport numpy as np\nfrom torch.utils.data import DataLoader, TensorDataset\n\ndef prepare_data(df_new):\n    # Define input and output features\n    input_features = ['Year', 'DOT_ID_Reporting_Airline', 'Tail_Number', 'Flight_Number_Reporting_Airline', \n                    'OriginAirportID', 'DestAirportID', 'CRSDepTime', 'DepartureDelayGroups', 'TaxiOut', \n                    'WheelsOff', 'WheelsOn', 'TaxiIn', 'CRSArrTime', 'CRSElapsedTime', 'Flights', 'Distance', \n                    'FirstDepTime', 'TotalAddGTime', 'LongestAddGTime', 'DivAirportLandings', 'DivActualElapsedTime', \n                    'DivArrDelay', 'DivDistance', 'Div1TotalGTime', 'Div1LongestGTime', 'Latitude', 'Longitude', \n                    'LatitudeDest', 'LongitudeDest', 'Airline_Ranking', 'tavg', 'tmin', 'tmax', 'prcp', 'snow', \n                    'wdir', 'wspd', 'pres', 'Month', 'Day']\n    output_features = ['DepTime', 'DepDelayMinutes', 'ArrTime', 'ArrDelayMinutes', 'Diverted', 'ActualElapsedTime', \n                    'AirTime', 'CarrierDelay', 'WeatherDelay', 'NASDelay', 'SecurityDelay', 'LateAircraftDelay', \n                    'CancellationCode_encoded']\n\n    # Separate categorical and continuous columns\n    categorical_cols = ['Tail_Number','OriginAirportID', 'DestAirportID']\n    continuous_cols = [col for col in input_features if col not in categorical_cols]\n\n    # Handle missing values\n    df_new = df_new.copy()\n    for col in continuous_cols:\n        df_new[col] = df_new[col].fillna(df_new[col].mean())\n    for col in categorical_cols:\n        df_new[col] = df_new[col].fillna('MISSING')\n    for col in output_features:\n        df_new[col] = df_new[col].fillna(0)\n\n    # Process categorical features\n    label_encoders = {}\n    categorical_data = []\n    vocab_sizes = []\n    \n    for col in categorical_cols:\n        # Add special tokens for padding and unknown\n        unique_values = df_new[col].astype(str).unique()\n        vocab_size = len(unique_values) + 2  # +2 for padding and unknown\n        vocab_sizes.append(vocab_size)\n        \n        # Fit label encoder\n        le = LabelEncoder()\n        encoded_values = le.fit_transform(unique_values)\n        label_encoders[col] = le\n        \n        # Transform data\n        transformed = le.transform(df_new[col].astype(str))\n        categorical_data.append(transformed)\n\n    # Convert categorical data to tensor format\n    categorical_tensor = torch.tensor(np.stack(categorical_data, axis=1), dtype=torch.long)\n    \n    # Normalize continuous features\n    scaler = StandardScaler()\n    continuous_tensor = torch.tensor(\n        scaler.fit_transform(df_new[continuous_cols].values), \n        dtype=torch.float32\n    )\n    \n    # Convert output features to tensor\n    output_tensor = torch.tensor(df_new[output_features].values, dtype=torch.float32)\n    \n    return continuous_tensor, categorical_tensor, output_tensor, continuous_cols, categorical_cols, vocab_sizes, label_encoders\n\nclass MultiTaskFlightDelayModel(nn.Module):\n    def __init__(self, continuous_dim, vocab_sizes, hidden_dim, num_outputs, embedding_dim=8):\n        super(MultiTaskFlightDelayModel, self).__init__()\n        \n        # Embeddings for categorical features\n        self.embeddings = nn.ModuleList([\n            nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n            for vocab_size in vocab_sizes\n        ])\n        \n        # Calculate total input dimension\n        total_embedding_dim = len(vocab_sizes) * embedding_dim\n        total_input_dim = continuous_dim + total_embedding_dim\n        \n        # Main network layers\n        self.network = nn.Sequential(\n            nn.Linear(total_input_dim, hidden_dim),\n            nn.BatchNorm1d(hidden_dim),\n            nn.ReLU(),\n            nn.Dropout(0.2),\n            nn.Linear(hidden_dim, hidden_dim),\n            nn.BatchNorm1d(hidden_dim),\n            nn.ReLU(),\n            nn.Dropout(0.2)\n        )\n        \n        # Task-specific output layers\n        self.output_layers = nn.ModuleList([\n            nn.Linear(hidden_dim, 1) for _ in range(num_outputs)\n        ])\n\n    def forward(self, continuous_x, categorical_x):\n        # Process categorical features\n        embeddings = []\n        for i, embedding_layer in enumerate(self.embeddings):\n            embedded = embedding_layer(categorical_x[:, i])\n            embeddings.append(embedded)\n        \n        # Concatenate all embeddings\n        embedded_categorical = torch.cat(embeddings, dim=1)\n        \n        # Concatenate with continuous features\n        combined_input = torch.cat([continuous_x, embedded_categorical], dim=1)\n        \n        # Forward pass through main network\n        shared_features = self.network(combined_input)\n        \n        # Get outputs for each task\n        outputs = [layer(shared_features) for layer in self.output_layers]\n        return torch.cat(outputs, dim=1)\n\nclass FlightDataset(torch.utils.data.Dataset):\n    def __init__(self, continuous_data, categorical_data, targets):\n        self.continuous_data = continuous_data\n        self.categorical_data = categorical_data\n        self.targets = targets\n\n    def __len__(self):\n        return len(self.targets)\n\n    def __getitem__(self, idx):\n        return (\n            self.continuous_data[idx],\n            self.categorical_data[idx],\n            self.targets[idx]\n        )\n\ndef train_model(model, train_loader, criterion, optimizer, device, num_epochs=20):\n    model.train()\n    for epoch in range(num_epochs):\n        total_loss = 0\n        for batch_idx, (continuous_x, categorical_x, targets) in enumerate(train_loader):\n            continuous_x = continuous_x.to(device)\n            categorical_x = categorical_x.to(device)\n            targets = targets.to(device)\n            \n            optimizer.zero_grad()\n            outputs = model(continuous_x, categorical_x)\n            loss = criterion(outputs, targets)\n            loss.backward()\n            optimizer.step()\n            \n            total_loss += loss.item()\n            \n            if (batch_idx + 1) % 100 == 0:\n                print(f'Epoch [{epoch+1}/{num_epochs}], Batch [{batch_idx+1}/{len(train_loader)}], Loss: {loss.item():.4f}')\n        \n        avg_loss = total_loss / len(train_loader)\n        print(f'Epoch [{epoch+1}/{num_epochs}], Average Loss: {avg_loss:.4f}')\n\ndef evaluate_model(model, test_loader, device):\n    model.eval()\n    predictions = []\n    actuals = []\n    \n    with torch.no_grad():\n        for continuous_x, categorical_x, targets in test_loader:\n            continuous_x = continuous_x.to(device)\n            categorical_x = categorical_x.to(device)\n            outputs = model(continuous_x, categorical_x)\n            predictions.append(outputs.cpu().numpy())\n            actuals.append(targets.cpu().numpy())\n    \n    predictions = np.concatenate(predictions)\n    actuals = np.concatenate(actuals)\n    rmse = np.sqrt(mean_squared_error(actuals, predictions))\n    return rmse\n\ndef main(df_new, batch_size=64, hidden_dim=128, learning_rate=0.001, num_epochs=30):\n    # Prepare data\n    continuous_data, categorical_data, targets, continuous_cols, categorical_cols, vocab_sizes, label_encoders = prepare_data(df_new)\n    \n    # Create dataset and dataloader\n    dataset = FlightDataset(continuous_data, categorical_data, targets)\n    data_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n    \n    # Setup device\n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    print(f\"Using device: {device}\")\n    \n    # Initialize model\n    model = MultiTaskFlightDelayModel(\n        continuous_dim=len(continuous_cols),\n        vocab_sizes=vocab_sizes,\n        hidden_dim=hidden_dim,\n        num_outputs=targets.shape[1]\n    ).to(device)\n    \n    # Setup training\n    criterion = nn.MSELoss()\n    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n    \n    # Train and evaluate\n    train_model(model, data_loader, criterion, optimizer, device, num_epochs)\n    rmse = evaluate_model(model, data_loader, device)\n    print(f'Final RMSE: {rmse:.4f}')\n    \n    return model, label_encoders\n\n# Usage example:\n\nmodel, label_encoders = main(df_new)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\ndf_new=pd.read_csv('/kaggle/input/dataaa/datasetfinal.csv',low_memory=False)\ndf_new['Month'] = pd.to_datetime(df_new['date_converted']).dt.month\ndf_new['Day']=pd.to_datetime(df_new['date_converted']).dt.day\ndf_new['Quarter'] = pd.to_datetime(df_new['date_converted']).dt.quarter\ndf_new['Week'] = pd.to_datetime(df_new['date_converted']).dt.isocalendar().week","metadata":{"execution":{"iopub.status.busy":"2024-10-30T10:15:37.245191Z","iopub.execute_input":"2024-10-30T10:15:37.245649Z","iopub.status.idle":"2024-10-30T10:15:44.834861Z","shell.execute_reply.started":"2024-10-30T10:15:37.245605Z","shell.execute_reply":"2024-10-30T10:15:44.833795Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"df_new=df_new.drop(columns=['date_converted','Unnamed: 0_x',])","metadata":{"execution":{"iopub.status.busy":"2024-10-30T10:16:10.187953Z","iopub.execute_input":"2024-10-30T10:16:10.188414Z","iopub.status.idle":"2024-10-30T10:16:10.264778Z","shell.execute_reply.started":"2024-10-30T10:16:10.188371Z","shell.execute_reply":"2024-10-30T10:16:10.263626Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"df_new=df_new.drop(columns=['FlightDate','OriginWac','DestWac','TotalAddGTime', 'LongestAddGTime', 'DivAirportLandings', 'DivActualElapsedTime', \n                    'DivArrDelay', 'DivDistance', 'Div1TotalGTime', 'Div1LongestGTime'])","metadata":{"execution":{"iopub.status.busy":"2024-10-30T10:43:34.460234Z","iopub.execute_input":"2024-10-30T10:43:34.460908Z","iopub.status.idle":"2024-10-30T10:43:34.536834Z","shell.execute_reply.started":"2024-10-30T10:43:34.460829Z","shell.execute_reply":"2024-10-30T10:43:34.535412Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"df_new.columns","metadata":{"execution":{"iopub.status.busy":"2024-10-30T10:43:38.734762Z","iopub.execute_input":"2024-10-30T10:43:38.735212Z","iopub.status.idle":"2024-10-30T10:43:38.744644Z","shell.execute_reply.started":"2024-10-30T10:43:38.735170Z","shell.execute_reply":"2024-10-30T10:43:38.743422Z"},"trusted":true},"execution_count":11,"outputs":[{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"Index(['Year', 'DOT_ID_Reporting_Airline', 'Tail_Number',\n       'Flight_Number_Reporting_Airline', 'OriginAirportID', 'OriginCityName',\n       'DestAirportID', 'DestCityName', 'CRSDepTime', 'DepTime',\n       'DepDelayMinutes', 'DepartureDelayGroups', 'TaxiOut', 'WheelsOff',\n       'WheelsOn', 'TaxiIn', 'CRSArrTime', 'ArrTime', 'ArrDelayMinutes',\n       'Diverted', 'CRSElapsedTime', 'ActualElapsedTime', 'AirTime', 'Flights',\n       'Distance', 'CarrierDelay', 'WeatherDelay', 'NASDelay', 'SecurityDelay',\n       'LateAircraftDelay', 'FirstDepTime', 'Latitude', 'Longitude',\n       'LatitudeDest', 'LongitudeDest', 'Airline_Ranking',\n       'CancellationCode_encoded', 'index', 'tavg', 'tmin', 'tmax', 'prcp',\n       'snow', 'wdir', 'wspd', 'pres', 'Month', 'Day', 'Quarter', 'Week'],\n      dtype='object')"},"metadata":{}}]},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom sklearn.preprocessing import LabelEncoder, StandardScaler\nfrom sklearn.metrics import mean_squared_error\nimport pandas as pd\nimport numpy as np\nfrom torch.utils.data import DataLoader, TensorDataset\n\ndef prepare_data(df_new):\n    # Define input and output features\n    input_features = ['Year', 'DOT_ID_Reporting_Airline', 'Tail_Number', 'Flight_Number_Reporting_Airline', \n                    'OriginAirportID','OriginCityName','DestAirportID','DestCityName', 'CRSDepTime', 'TaxiOut', \n                    'WheelsOff', 'WheelsOn', 'TaxiIn', 'CRSArrTime', 'CRSElapsedTime', 'Flights', 'Distance', \n                      'Latitude', 'Longitude',  'LatitudeDest', 'LongitudeDest', 'Airline_Ranking', 'tavg', 'tmin', 'tmax', 'prcp', 'snow', \n                    'wdir', 'wspd', 'pres', 'Month', 'Day','Quarter','Week']\n    output_features = ['DepTime', 'DepDelayMinutes','DepartureDelayGroups', 'ArrTime', 'ArrDelayMinutes', 'Diverted', 'ActualElapsedTime', \n                    'AirTime', 'CarrierDelay', 'WeatherDelay', 'NASDelay', 'SecurityDelay', 'LateAircraftDelay', \n                    'CancellationCode_encoded']\n\n    # Separate categorical and continuous columns\n    categorical_cols = ['Tail_Number','OriginCityName', 'DestCityName']\n    continuous_cols = [col for col in input_features if col not in categorical_cols]\n\n    # Handle missing values\n    df_new = df_new.copy()\n    for col in continuous_cols:\n        df_new[col] = df_new[col].fillna(df_new[col].mean())\n    for col in categorical_cols:\n        df_new[col] = df_new[col].fillna('MISSING')\n    for col in output_features:\n        df_new[col] = df_new[col].fillna(0)\n\n    # Process categorical features\n    label_encoders = {}\n    categorical_data = []\n    vocab_sizes = []\n    \n    for col in categorical_cols:\n        # Add special tokens for padding and unknown\n        unique_values = df_new[col].astype(str).unique()\n        vocab_size = len(unique_values) + 2  # +2 for padding and unknown\n        vocab_sizes.append(vocab_size)\n        \n        # Fit label encoder\n        le = LabelEncoder()\n        encoded_values = le.fit_transform(unique_values)\n        label_encoders[col] = le\n        \n        # Transform data\n        transformed = le.transform(df_new[col].astype(str))\n        categorical_data.append(transformed)\n\n    # Convert categorical data to tensor format\n    categorical_tensor = torch.tensor(np.stack(categorical_data, axis=1), dtype=torch.long)\n    \n    # Normalize continuous features\n    scaler = StandardScaler()\n    continuous_tensor = torch.tensor(\n        scaler.fit_transform(df_new[continuous_cols].values), \n        dtype=torch.float32\n    )\n    \n    # Convert output features to tensor\n    output_tensor = torch.tensor(df_new[output_features].values, dtype=torch.float32)\n    \n    return continuous_tensor, categorical_tensor, output_tensor, continuous_cols, categorical_cols, vocab_sizes, label_encoders\n\nclass MultiTaskFlightDelayModel(nn.Module):\n    def __init__(self, continuous_dim, vocab_sizes, hidden_dim, num_outputs, embedding_dim=8):\n        super(MultiTaskFlightDelayModel, self).__init__()\n        \n        # Embeddings for categorical features\n        self.embeddings = nn.ModuleList([\n            nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n            for vocab_size in vocab_sizes\n        ])\n        \n        # Calculate total input dimension\n        total_embedding_dim = len(vocab_sizes) * embedding_dim\n        total_input_dim = continuous_dim + total_embedding_dim\n        \n        # Main network layers\n        self.network = nn.Sequential(\n            nn.Linear(total_input_dim, hidden_dim),\n            nn.BatchNorm1d(hidden_dim),\n            nn.ReLU(),\n            nn.Dropout(0.2),\n            nn.Linear(hidden_dim, hidden_dim),\n            nn.BatchNorm1d(hidden_dim),\n            nn.ReLU(),\n            nn.Dropout(0.2)\n        )\n        \n        # Task-specific output layers\n        self.output_layers = nn.ModuleList([\n            nn.Linear(hidden_dim, 1) for _ in range(num_outputs)\n        ])\n\n    def forward(self, continuous_x, categorical_x):\n        # Process categorical features\n        embeddings = []\n        for i, embedding_layer in enumerate(self.embeddings):\n            embedded = embedding_layer(categorical_x[:, i])\n            embeddings.append(embedded)\n        \n        # Concatenate all embeddings\n        embedded_categorical = torch.cat(embeddings, dim=1)\n        \n        # Concatenate with continuous features\n        combined_input = torch.cat([continuous_x, embedded_categorical], dim=1)\n        \n        # Forward pass through main network\n        shared_features = self.network(combined_input)\n        \n        # Get outputs for each task\n        outputs = [layer(shared_features) for layer in self.output_layers]\n        return torch.cat(outputs, dim=1)\n\nclass FlightDataset(torch.utils.data.Dataset):\n    def __init__(self, continuous_data, categorical_data, targets):\n        self.continuous_data = continuous_data\n        self.categorical_data = categorical_data\n        self.targets = targets\n\n    def __len__(self):\n        return len(self.targets)\n\n    def __getitem__(self, idx):\n        return (\n            self.continuous_data[idx],\n            self.categorical_data[idx],\n            self.targets[idx]\n        )\n\ndef train_model(model, train_loader, criterion, optimizer, device, num_epochs=20):\n    model.train()\n    for epoch in range(num_epochs):\n        total_loss = 0\n        for batch_idx, (continuous_x, categorical_x, targets) in enumerate(train_loader):\n            continuous_x = continuous_x.to(device)\n            categorical_x = categorical_x.to(device)\n            targets = targets.to(device)\n            \n            optimizer.zero_grad()\n            outputs = model(continuous_x, categorical_x)\n            loss = criterion(outputs, targets)\n            loss.backward()\n            optimizer.step()\n            \n            total_loss += loss.item()\n            \n            if (batch_idx + 1) % 100 == 0:\n                print(f'Epoch [{epoch+1}/{num_epochs}], Batch [{batch_idx+1}/{len(train_loader)}], Loss: {loss.item():.4f}')\n        \n        avg_loss = total_loss / len(train_loader)\n        print(f'Epoch [{epoch+1}/{num_epochs}], Average Loss: {avg_loss:.4f}')\n\ndef evaluate_model(model, test_loader, device):\n    model.eval()\n    predictions = []\n    actuals = []\n    \n    with torch.no_grad():\n        for continuous_x, categorical_x, targets in test_loader:\n            continuous_x = continuous_x.to(device)\n            categorical_x = categorical_x.to(device)\n            outputs = model(continuous_x, categorical_x)\n            predictions.append(outputs.cpu().numpy())\n            actuals.append(targets.cpu().numpy())\n    \n    predictions = np.concatenate(predictions)\n    actuals = np.concatenate(actuals)\n    rmse = np.sqrt(mean_squared_error(actuals, predictions))\n    return rmse\n\ndef main(df_new, batch_size=64, hidden_dim=128, learning_rate=0.001, num_epochs=20):\n    # Prepare data\n    continuous_data, categorical_data, targets, continuous_cols, categorical_cols, vocab_sizes, label_encoders = prepare_data(df_new)\n    \n    # Create dataset and dataloader\n    dataset = FlightDataset(continuous_data, categorical_data, targets)\n    data_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n    \n    # Setup device\n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    print(f\"Using device: {device}\")\n    \n    # Initialize model\n    model = MultiTaskFlightDelayModel(\n        continuous_dim=len(continuous_cols),\n        vocab_sizes=vocab_sizes,\n        hidden_dim=hidden_dim,\n        num_outputs=targets.shape[1]\n    ).to(device)\n    \n    # Setup training\n    criterion = nn.MSELoss()\n    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n    \n    # Train and evaluate\n    train_model(model, data_loader, criterion, optimizer, device, num_epochs)\n    rmse = evaluate_model(model, data_loader, device)\n    print(f'Final RMSE: {rmse:.4f}')\n    \n    return model, label_encoders\n\n# Usage example:\nmodel, label_encoders = main(train_df)","metadata":{"execution":{"iopub.status.busy":"2024-10-30T10:53:56.022761Z","iopub.execute_input":"2024-10-30T10:53:56.023801Z","iopub.status.idle":"2024-10-30T11:07:32.961933Z","shell.execute_reply.started":"2024-10-30T10:53:56.023754Z","shell.execute_reply":"2024-10-30T11:07:32.960720Z"},"trusted":true},"execution_count":15,"outputs":[{"name":"stdout","text":"Using device: cpu\nEpoch [1/20], Batch [100/5108], Loss: 325661.9688\nEpoch [1/20], Batch [200/5108], Loss: 323567.1875\nEpoch [1/20], Batch [300/5108], Loss: 339483.5938\nEpoch [1/20], Batch [400/5108], Loss: 290737.0312\nEpoch [1/20], Batch [500/5108], Loss: 269453.4062\nEpoch [1/20], Batch [600/5108], Loss: 265553.9375\nEpoch [1/20], Batch [700/5108], Loss: 243443.4531\nEpoch [1/20], Batch [800/5108], Loss: 218849.5781\nEpoch [1/20], Batch [900/5108], Loss: 213753.7500\nEpoch [1/20], Batch [1000/5108], Loss: 219944.2656\nEpoch [1/20], Batch [1100/5108], Loss: 199496.1094\nEpoch [1/20], Batch [1200/5108], Loss: 162956.5469\nEpoch [1/20], Batch [1300/5108], Loss: 133247.1719\nEpoch [1/20], Batch [1400/5108], Loss: 107091.9375\nEpoch [1/20], Batch [1500/5108], Loss: 110231.3750\nEpoch [1/20], Batch [1600/5108], Loss: 102975.6953\nEpoch [1/20], Batch [1700/5108], Loss: 68521.2656\nEpoch [1/20], Batch [1800/5108], Loss: 76828.3359\nEpoch [1/20], Batch [1900/5108], Loss: 62334.5938\nEpoch [1/20], Batch [2000/5108], Loss: 50544.9336\nEpoch [1/20], Batch [2100/5108], Loss: 44213.4492\nEpoch [1/20], Batch [2200/5108], Loss: 57039.2227\nEpoch [1/20], Batch [2300/5108], Loss: 44757.1250\nEpoch [1/20], Batch [2400/5108], Loss: 54135.7852\nEpoch [1/20], Batch [2500/5108], Loss: 51232.8828\nEpoch [1/20], Batch [2600/5108], Loss: 50062.6172\nEpoch [1/20], Batch [2700/5108], Loss: 22583.4902\nEpoch [1/20], Batch [2800/5108], Loss: 21861.3691\nEpoch [1/20], Batch [2900/5108], Loss: 24401.9336\nEpoch [1/20], Batch [3000/5108], Loss: 12691.3682\nEpoch [1/20], Batch [3100/5108], Loss: 14147.2354\nEpoch [1/20], Batch [3200/5108], Loss: 20345.3184\nEpoch [1/20], Batch [3300/5108], Loss: 15694.3691\nEpoch [1/20], Batch [3400/5108], Loss: 11537.8662\nEpoch [1/20], Batch [3500/5108], Loss: 9232.8730\nEpoch [1/20], Batch [3600/5108], Loss: 17796.2109\nEpoch [1/20], Batch [3700/5108], Loss: 8939.0557\nEpoch [1/20], Batch [3800/5108], Loss: 5711.5981\nEpoch [1/20], Batch [3900/5108], Loss: 13518.5830\nEpoch [1/20], Batch [4000/5108], Loss: 8132.1577\nEpoch [1/20], Batch [4100/5108], Loss: 16863.2305\nEpoch [1/20], Batch [4200/5108], Loss: 13120.5264\nEpoch [1/20], Batch [4300/5108], Loss: 5402.4331\nEpoch [1/20], Batch [4400/5108], Loss: 8811.7266\nEpoch [1/20], Batch [4500/5108], Loss: 5453.4038\nEpoch [1/20], Batch [4600/5108], Loss: 8718.5254\nEpoch [1/20], Batch [4700/5108], Loss: 12180.2393\nEpoch [1/20], Batch [4800/5108], Loss: 3925.7949\nEpoch [1/20], Batch [4900/5108], Loss: 6143.8271\nEpoch [1/20], Batch [5000/5108], Loss: 6764.9819\nEpoch [1/20], Batch [5100/5108], Loss: 3894.1548\nEpoch [1/20], Average Loss: 87176.3789\nEpoch [2/20], Batch [100/5108], Loss: 4833.0791\nEpoch [2/20], Batch [200/5108], Loss: 5267.4565\nEpoch [2/20], Batch [300/5108], Loss: 4570.8135\nEpoch [2/20], Batch [400/5108], Loss: 5954.3838\nEpoch [2/20], Batch [500/5108], Loss: 2651.5254\nEpoch [2/20], Batch [600/5108], Loss: 3128.4258\nEpoch [2/20], Batch [700/5108], Loss: 4065.6050\nEpoch [2/20], Batch [800/5108], Loss: 5310.8296\nEpoch [2/20], Batch [900/5108], Loss: 4886.1763\nEpoch [2/20], Batch [1000/5108], Loss: 3079.9202\nEpoch [2/20], Batch [1100/5108], Loss: 5803.7549\nEpoch [2/20], Batch [1200/5108], Loss: 2542.5510\nEpoch [2/20], Batch [1300/5108], Loss: 3119.1948\nEpoch [2/20], Batch [1400/5108], Loss: 7514.1919\nEpoch [2/20], Batch [1500/5108], Loss: 5421.3950\nEpoch [2/20], Batch [1600/5108], Loss: 3503.9106\nEpoch [2/20], Batch [1700/5108], Loss: 3448.8113\nEpoch [2/20], Batch [1800/5108], Loss: 2184.3796\nEpoch [2/20], Batch [1900/5108], Loss: 3580.8755\nEpoch [2/20], Batch [2000/5108], Loss: 4449.8911\nEpoch [2/20], Batch [2100/5108], Loss: 4588.8438\nEpoch [2/20], Batch [2200/5108], Loss: 3109.4331\nEpoch [2/20], Batch [2300/5108], Loss: 3596.7188\nEpoch [2/20], Batch [2400/5108], Loss: 3489.1362\nEpoch [2/20], Batch [2500/5108], Loss: 2857.3621\nEpoch [2/20], Batch [2600/5108], Loss: 4123.5195\nEpoch [2/20], Batch [2700/5108], Loss: 2191.6497\nEpoch [2/20], Batch [2800/5108], Loss: 5804.1997\nEpoch [2/20], Batch [2900/5108], Loss: 6289.4443\nEpoch [2/20], Batch [3000/5108], Loss: 6614.4375\nEpoch [2/20], Batch [3100/5108], Loss: 3889.2246\nEpoch [2/20], Batch [3200/5108], Loss: 2527.3103\nEpoch [2/20], Batch [3300/5108], Loss: 4471.3037\nEpoch [2/20], Batch [3400/5108], Loss: 3378.6782\nEpoch [2/20], Batch [3500/5108], Loss: 2913.3130\nEpoch [2/20], Batch [3600/5108], Loss: 2659.8179\nEpoch [2/20], Batch [3700/5108], Loss: 2769.0918\nEpoch [2/20], Batch [3800/5108], Loss: 3282.1157\nEpoch [2/20], Batch [3900/5108], Loss: 3638.8242\nEpoch [2/20], Batch [4000/5108], Loss: 3149.9426\nEpoch [2/20], Batch [4100/5108], Loss: 3866.9336\nEpoch [2/20], Batch [4200/5108], Loss: 2337.3992\nEpoch [2/20], Batch [4300/5108], Loss: 2939.6101\nEpoch [2/20], Batch [4400/5108], Loss: 2209.5139\nEpoch [2/20], Batch [4500/5108], Loss: 3515.2827\nEpoch [2/20], Batch [4600/5108], Loss: 3846.5334\nEpoch [2/20], Batch [4700/5108], Loss: 2802.5505\nEpoch [2/20], Batch [4800/5108], Loss: 4781.8018\nEpoch [2/20], Batch [4900/5108], Loss: 2847.5476\nEpoch [2/20], Batch [5000/5108], Loss: 4891.1343\nEpoch [2/20], Batch [5100/5108], Loss: 3219.1277\nEpoch [2/20], Average Loss: 3936.6153\nEpoch [3/20], Batch [100/5108], Loss: 2372.3242\nEpoch [3/20], Batch [200/5108], Loss: 3290.5540\nEpoch [3/20], Batch [300/5108], Loss: 2415.0320\nEpoch [3/20], Batch [400/5108], Loss: 2843.6277\nEpoch [3/20], Batch [500/5108], Loss: 2382.5330\nEpoch [3/20], Batch [600/5108], Loss: 2467.4512\nEpoch [3/20], Batch [700/5108], Loss: 3498.6284\nEpoch [3/20], Batch [800/5108], Loss: 1665.4403\nEpoch [3/20], Batch [900/5108], Loss: 6880.7578\nEpoch [3/20], Batch [1000/5108], Loss: 2579.1450\nEpoch [3/20], Batch [1100/5108], Loss: 2931.6177\nEpoch [3/20], Batch [1200/5108], Loss: 3586.1621\nEpoch [3/20], Batch [1300/5108], Loss: 2063.0530\nEpoch [3/20], Batch [1400/5108], Loss: 2884.3733\nEpoch [3/20], Batch [1500/5108], Loss: 3019.9016\nEpoch [3/20], Batch [1600/5108], Loss: 2475.6140\nEpoch [3/20], Batch [1700/5108], Loss: 2859.5000\nEpoch [3/20], Batch [1800/5108], Loss: 3069.6372\nEpoch [3/20], Batch [1900/5108], Loss: 3016.0581\nEpoch [3/20], Batch [2000/5108], Loss: 3199.3003\nEpoch [3/20], Batch [2100/5108], Loss: 2509.0496\nEpoch [3/20], Batch [2200/5108], Loss: 2228.8943\nEpoch [3/20], Batch [2300/5108], Loss: 2350.0098\nEpoch [3/20], Batch [2400/5108], Loss: 2545.2578\nEpoch [3/20], Batch [2500/5108], Loss: 3625.4797\nEpoch [3/20], Batch [2600/5108], Loss: 2194.6028\nEpoch [3/20], Batch [2700/5108], Loss: 2781.4263\nEpoch [3/20], Batch [2800/5108], Loss: 2673.6216\nEpoch [3/20], Batch [2900/5108], Loss: 2558.8347\nEpoch [3/20], Batch [3000/5108], Loss: 3312.3628\nEpoch [3/20], Batch [3100/5108], Loss: 4244.9233\nEpoch [3/20], Batch [3200/5108], Loss: 2004.9543\nEpoch [3/20], Batch [3300/5108], Loss: 3205.1206\nEpoch [3/20], Batch [3400/5108], Loss: 2661.2280\nEpoch [3/20], Batch [3500/5108], Loss: 1812.4957\nEpoch [3/20], Batch [3600/5108], Loss: 1925.9691\nEpoch [3/20], Batch [3700/5108], Loss: 2030.2617\nEpoch [3/20], Batch [3800/5108], Loss: 2642.7612\nEpoch [3/20], Batch [3900/5108], Loss: 3520.6816\nEpoch [3/20], Batch [4000/5108], Loss: 4143.3096\nEpoch [3/20], Batch [4100/5108], Loss: 3694.9565\nEpoch [3/20], Batch [4200/5108], Loss: 2443.1863\nEpoch [3/20], Batch [4300/5108], Loss: 2923.1809\nEpoch [3/20], Batch [4400/5108], Loss: 2480.7058\nEpoch [3/20], Batch [4500/5108], Loss: 2760.9341\nEpoch [3/20], Batch [4600/5108], Loss: 2819.2390\nEpoch [3/20], Batch [4700/5108], Loss: 2259.6304\nEpoch [3/20], Batch [4800/5108], Loss: 3154.2148\nEpoch [3/20], Batch [4900/5108], Loss: 3040.4304\nEpoch [3/20], Batch [5000/5108], Loss: 3140.9973\nEpoch [3/20], Batch [5100/5108], Loss: 2095.4077\nEpoch [3/20], Average Loss: 2997.2831\nEpoch [4/20], Batch [100/5108], Loss: 2869.4512\nEpoch [4/20], Batch [200/5108], Loss: 2221.8821\nEpoch [4/20], Batch [300/5108], Loss: 2767.2141\nEpoch [4/20], Batch [400/5108], Loss: 2221.3428\nEpoch [4/20], Batch [500/5108], Loss: 3302.8550\nEpoch [4/20], Batch [600/5108], Loss: 3417.8064\nEpoch [4/20], Batch [700/5108], Loss: 2735.2942\nEpoch [4/20], Batch [800/5108], Loss: 2550.0493\nEpoch [4/20], Batch [900/5108], Loss: 3865.2107\nEpoch [4/20], Batch [1000/5108], Loss: 2479.4272\nEpoch [4/20], Batch [1100/5108], Loss: 3337.3508\nEpoch [4/20], Batch [1200/5108], Loss: 2527.1580\nEpoch [4/20], Batch [1300/5108], Loss: 2989.4116\nEpoch [4/20], Batch [1400/5108], Loss: 2477.9114\nEpoch [4/20], Batch [1500/5108], Loss: 1957.7742\nEpoch [4/20], Batch [1600/5108], Loss: 2355.2305\nEpoch [4/20], Batch [1700/5108], Loss: 2788.9919\nEpoch [4/20], Batch [1800/5108], Loss: 3857.1785\nEpoch [4/20], Batch [1900/5108], Loss: 4436.9448\nEpoch [4/20], Batch [2000/5108], Loss: 2368.5454\nEpoch [4/20], Batch [2100/5108], Loss: 2746.8191\nEpoch [4/20], Batch [2200/5108], Loss: 3613.6311\nEpoch [4/20], Batch [2300/5108], Loss: 2201.8145\nEpoch [4/20], Batch [2400/5108], Loss: 2084.9412\nEpoch [4/20], Batch [2500/5108], Loss: 2714.3208\nEpoch [4/20], Batch [2600/5108], Loss: 2198.5330\nEpoch [4/20], Batch [2700/5108], Loss: 4192.4771\nEpoch [4/20], Batch [2800/5108], Loss: 1880.4120\nEpoch [4/20], Batch [2900/5108], Loss: 2390.3633\nEpoch [4/20], Batch [3000/5108], Loss: 2265.8750\nEpoch [4/20], Batch [3100/5108], Loss: 1665.2267\nEpoch [4/20], Batch [3200/5108], Loss: 2420.7710\nEpoch [4/20], Batch [3300/5108], Loss: 2630.0886\nEpoch [4/20], Batch [3400/5108], Loss: 3342.5085\nEpoch [4/20], Batch [3500/5108], Loss: 2215.0081\nEpoch [4/20], Batch [3600/5108], Loss: 2739.3933\nEpoch [4/20], Batch [3700/5108], Loss: 2956.0532\nEpoch [4/20], Batch [3800/5108], Loss: 3877.4263\nEpoch [4/20], Batch [3900/5108], Loss: 2865.3616\nEpoch [4/20], Batch [4000/5108], Loss: 2094.1887\nEpoch [4/20], Batch [4100/5108], Loss: 2946.8445\nEpoch [4/20], Batch [4200/5108], Loss: 2078.1118\nEpoch [4/20], Batch [4300/5108], Loss: 2251.0142\nEpoch [4/20], Batch [4400/5108], Loss: 3208.2258\nEpoch [4/20], Batch [4500/5108], Loss: 2476.6106\nEpoch [4/20], Batch [4600/5108], Loss: 4990.6265\nEpoch [4/20], Batch [4700/5108], Loss: 1921.5936\nEpoch [4/20], Batch [4800/5108], Loss: 1845.1257\nEpoch [4/20], Batch [4900/5108], Loss: 1872.6328\nEpoch [4/20], Batch [5000/5108], Loss: 2700.8208\nEpoch [4/20], Batch [5100/5108], Loss: 3708.1692\nEpoch [4/20], Average Loss: 2725.1762\nEpoch [5/20], Batch [100/5108], Loss: 1974.5002\nEpoch [5/20], Batch [200/5108], Loss: 2705.7759\nEpoch [5/20], Batch [300/5108], Loss: 2490.5374\nEpoch [5/20], Batch [400/5108], Loss: 2466.6296\nEpoch [5/20], Batch [500/5108], Loss: 3062.5847\nEpoch [5/20], Batch [600/5108], Loss: 2691.5020\nEpoch [5/20], Batch [700/5108], Loss: 2054.7512\nEpoch [5/20], Batch [800/5108], Loss: 2236.3115\nEpoch [5/20], Batch [900/5108], Loss: 2797.9548\nEpoch [5/20], Batch [1000/5108], Loss: 2232.5769\nEpoch [5/20], Batch [1100/5108], Loss: 1948.3391\nEpoch [5/20], Batch [1200/5108], Loss: 1874.6318\nEpoch [5/20], Batch [1300/5108], Loss: 2984.7769\nEpoch [5/20], Batch [1400/5108], Loss: 2945.8970\nEpoch [5/20], Batch [1500/5108], Loss: 3068.3335\nEpoch [5/20], Batch [1600/5108], Loss: 2970.6055\nEpoch [5/20], Batch [1700/5108], Loss: 3381.8469\nEpoch [5/20], Batch [1800/5108], Loss: 1682.8170\nEpoch [5/20], Batch [1900/5108], Loss: 2181.0225\nEpoch [5/20], Batch [2000/5108], Loss: 2254.4033\nEpoch [5/20], Batch [2100/5108], Loss: 1937.4884\nEpoch [5/20], Batch [2200/5108], Loss: 2487.6853\nEpoch [5/20], Batch [2300/5108], Loss: 2135.2590\nEpoch [5/20], Batch [2400/5108], Loss: 2175.6316\nEpoch [5/20], Batch [2500/5108], Loss: 2067.6086\nEpoch [5/20], Batch [2600/5108], Loss: 1836.8800\nEpoch [5/20], Batch [2700/5108], Loss: 1699.2963\nEpoch [5/20], Batch [2800/5108], Loss: 2172.7671\nEpoch [5/20], Batch [2900/5108], Loss: 2376.8328\nEpoch [5/20], Batch [3000/5108], Loss: 2698.9546\nEpoch [5/20], Batch [3100/5108], Loss: 1733.7999\nEpoch [5/20], Batch [3200/5108], Loss: 2616.7644\nEpoch [5/20], Batch [3300/5108], Loss: 3412.3264\nEpoch [5/20], Batch [3400/5108], Loss: 2589.2244\nEpoch [5/20], Batch [3500/5108], Loss: 1807.0383\nEpoch [5/20], Batch [3600/5108], Loss: 2303.7385\nEpoch [5/20], Batch [3700/5108], Loss: 1710.1536\nEpoch [5/20], Batch [3800/5108], Loss: 2499.5728\nEpoch [5/20], Batch [3900/5108], Loss: 2268.5833\nEpoch [5/20], Batch [4000/5108], Loss: 3197.7271\nEpoch [5/20], Batch [4100/5108], Loss: 3259.2732\nEpoch [5/20], Batch [4200/5108], Loss: 3187.2090\nEpoch [5/20], Batch [4300/5108], Loss: 2109.4302\nEpoch [5/20], Batch [4400/5108], Loss: 2199.7234\nEpoch [5/20], Batch [4500/5108], Loss: 2740.7866\nEpoch [5/20], Batch [4600/5108], Loss: 1657.7336\nEpoch [5/20], Batch [4700/5108], Loss: 2361.3596\nEpoch [5/20], Batch [4800/5108], Loss: 2099.3411\nEpoch [5/20], Batch [4900/5108], Loss: 3147.8020\nEpoch [5/20], Batch [5000/5108], Loss: 2064.4453\nEpoch [5/20], Batch [5100/5108], Loss: 2002.7401\nEpoch [5/20], Average Loss: 2558.1417\nEpoch [6/20], Batch [100/5108], Loss: 2284.4360\nEpoch [6/20], Batch [200/5108], Loss: 2646.6956\nEpoch [6/20], Batch [300/5108], Loss: 4100.4756\nEpoch [6/20], Batch [400/5108], Loss: 2223.3418\nEpoch [6/20], Batch [500/5108], Loss: 2322.3289\nEpoch [6/20], Batch [600/5108], Loss: 1967.3976\nEpoch [6/20], Batch [700/5108], Loss: 1948.9840\nEpoch [6/20], Batch [800/5108], Loss: 1826.1061\nEpoch [6/20], Batch [900/5108], Loss: 2582.0825\nEpoch [6/20], Batch [1000/5108], Loss: 2617.1812\nEpoch [6/20], Batch [1100/5108], Loss: 2148.9946\nEpoch [6/20], Batch [1200/5108], Loss: 2483.3513\nEpoch [6/20], Batch [1300/5108], Loss: 3520.7195\nEpoch [6/20], Batch [1400/5108], Loss: 2538.2239\nEpoch [6/20], Batch [1500/5108], Loss: 2724.2910\nEpoch [6/20], Batch [1600/5108], Loss: 1834.6180\nEpoch [6/20], Batch [1700/5108], Loss: 2103.2053\nEpoch [6/20], Batch [1800/5108], Loss: 3634.0991\nEpoch [6/20], Batch [1900/5108], Loss: 2071.5867\nEpoch [6/20], Batch [2000/5108], Loss: 2009.6803\nEpoch [6/20], Batch [2100/5108], Loss: 2583.2703\nEpoch [6/20], Batch [2200/5108], Loss: 2626.2427\nEpoch [6/20], Batch [2300/5108], Loss: 2128.3699\nEpoch [6/20], Batch [2400/5108], Loss: 2632.2195\nEpoch [6/20], Batch [2500/5108], Loss: 2871.4648\nEpoch [6/20], Batch [2600/5108], Loss: 2048.7458\nEpoch [6/20], Batch [2700/5108], Loss: 2778.4451\nEpoch [6/20], Batch [2800/5108], Loss: 2464.2283\nEpoch [6/20], Batch [2900/5108], Loss: 2206.7527\nEpoch [6/20], Batch [3000/5108], Loss: 2006.2932\nEpoch [6/20], Batch [3100/5108], Loss: 2653.8425\nEpoch [6/20], Batch [3200/5108], Loss: 2797.4045\nEpoch [6/20], Batch [3300/5108], Loss: 1870.9570\nEpoch [6/20], Batch [3400/5108], Loss: 2615.9751\nEpoch [6/20], Batch [3500/5108], Loss: 3457.8547\nEpoch [6/20], Batch [3600/5108], Loss: 1895.4384\nEpoch [6/20], Batch [3700/5108], Loss: 1812.0720\nEpoch [6/20], Batch [3800/5108], Loss: 2584.6753\nEpoch [6/20], Batch [3900/5108], Loss: 2870.0408\nEpoch [6/20], Batch [4000/5108], Loss: 3492.2209\nEpoch [6/20], Batch [4100/5108], Loss: 2200.8674\nEpoch [6/20], Batch [4200/5108], Loss: 2700.4265\nEpoch [6/20], Batch [4300/5108], Loss: 2753.7534\nEpoch [6/20], Batch [4400/5108], Loss: 1963.5872\nEpoch [6/20], Batch [4500/5108], Loss: 2201.1997\nEpoch [6/20], Batch [4600/5108], Loss: 2137.5347\nEpoch [6/20], Batch [4700/5108], Loss: 3342.9419\nEpoch [6/20], Batch [4800/5108], Loss: 1998.0518\nEpoch [6/20], Batch [4900/5108], Loss: 2294.0076\nEpoch [6/20], Batch [5000/5108], Loss: 1319.5791\nEpoch [6/20], Batch [5100/5108], Loss: 2085.2556\nEpoch [6/20], Average Loss: 2457.9053\nEpoch [7/20], Batch [100/5108], Loss: 1595.1464\nEpoch [7/20], Batch [200/5108], Loss: 2440.4285\nEpoch [7/20], Batch [300/5108], Loss: 1757.7385\nEpoch [7/20], Batch [400/5108], Loss: 2776.0466\nEpoch [7/20], Batch [500/5108], Loss: 4089.8130\nEpoch [7/20], Batch [600/5108], Loss: 3133.6895\nEpoch [7/20], Batch [700/5108], Loss: 2805.6077\nEpoch [7/20], Batch [800/5108], Loss: 2103.1326\nEpoch [7/20], Batch [900/5108], Loss: 1969.5482\nEpoch [7/20], Batch [1000/5108], Loss: 1909.7031\nEpoch [7/20], Batch [1100/5108], Loss: 3381.5076\nEpoch [7/20], Batch [1200/5108], Loss: 4892.9976\nEpoch [7/20], Batch [1300/5108], Loss: 2768.0151\nEpoch [7/20], Batch [1400/5108], Loss: 3208.4961\nEpoch [7/20], Batch [1500/5108], Loss: 2076.3381\nEpoch [7/20], Batch [1600/5108], Loss: 2219.4341\nEpoch [7/20], Batch [1700/5108], Loss: 1847.4443\nEpoch [7/20], Batch [1800/5108], Loss: 2491.4526\nEpoch [7/20], Batch [1900/5108], Loss: 1664.8802\nEpoch [7/20], Batch [2000/5108], Loss: 2658.2988\nEpoch [7/20], Batch [2100/5108], Loss: 2668.6941\nEpoch [7/20], Batch [2200/5108], Loss: 2786.0696\nEpoch [7/20], Batch [2300/5108], Loss: 1844.7100\nEpoch [7/20], Batch [2400/5108], Loss: 2680.1130\nEpoch [7/20], Batch [2500/5108], Loss: 1529.9524\nEpoch [7/20], Batch [2600/5108], Loss: 2007.6404\nEpoch [7/20], Batch [2700/5108], Loss: 3054.6663\nEpoch [7/20], Batch [2800/5108], Loss: 1859.4801\nEpoch [7/20], Batch [2900/5108], Loss: 1824.6539\nEpoch [7/20], Batch [3000/5108], Loss: 2360.0535\nEpoch [7/20], Batch [3100/5108], Loss: 1948.8024\nEpoch [7/20], Batch [3200/5108], Loss: 1527.6819\nEpoch [7/20], Batch [3300/5108], Loss: 2025.8451\nEpoch [7/20], Batch [3400/5108], Loss: 1854.7246\nEpoch [7/20], Batch [3500/5108], Loss: 1843.6611\nEpoch [7/20], Batch [3600/5108], Loss: 3577.6096\nEpoch [7/20], Batch [3700/5108], Loss: 2066.9148\nEpoch [7/20], Batch [3800/5108], Loss: 2935.5374\nEpoch [7/20], Batch [3900/5108], Loss: 2476.5168\nEpoch [7/20], Batch [4000/5108], Loss: 1980.8376\nEpoch [7/20], Batch [4100/5108], Loss: 2871.3201\nEpoch [7/20], Batch [4200/5108], Loss: 2641.5676\nEpoch [7/20], Batch [4300/5108], Loss: 2633.4546\nEpoch [7/20], Batch [4400/5108], Loss: 4082.1934\nEpoch [7/20], Batch [4500/5108], Loss: 2475.4966\nEpoch [7/20], Batch [4600/5108], Loss: 1575.7712\nEpoch [7/20], Batch [4700/5108], Loss: 2658.1646\nEpoch [7/20], Batch [4800/5108], Loss: 1962.1820\nEpoch [7/20], Batch [4900/5108], Loss: 3081.7559\nEpoch [7/20], Batch [5000/5108], Loss: 2838.7380\nEpoch [7/20], Batch [5100/5108], Loss: 1553.6121\nEpoch [7/20], Average Loss: 2394.6839\nEpoch [8/20], Batch [100/5108], Loss: 1871.0605\nEpoch [8/20], Batch [200/5108], Loss: 3266.4858\nEpoch [8/20], Batch [300/5108], Loss: 3131.9724\nEpoch [8/20], Batch [400/5108], Loss: 2354.9785\nEpoch [8/20], Batch [500/5108], Loss: 2969.5859\nEpoch [8/20], Batch [600/5108], Loss: 2431.3518\nEpoch [8/20], Batch [700/5108], Loss: 2098.2490\nEpoch [8/20], Batch [800/5108], Loss: 1953.0404\nEpoch [8/20], Batch [900/5108], Loss: 1667.4247\nEpoch [8/20], Batch [1000/5108], Loss: 3608.7678\nEpoch [8/20], Batch [1100/5108], Loss: 1954.7345\nEpoch [8/20], Batch [1200/5108], Loss: 2653.0859\nEpoch [8/20], Batch [1300/5108], Loss: 2052.1345\nEpoch [8/20], Batch [1400/5108], Loss: 2436.5034\nEpoch [8/20], Batch [1500/5108], Loss: 2612.8342\nEpoch [8/20], Batch [1600/5108], Loss: 1374.4247\nEpoch [8/20], Batch [1700/5108], Loss: 2248.9800\nEpoch [8/20], Batch [1800/5108], Loss: 1821.5885\nEpoch [8/20], Batch [1900/5108], Loss: 1832.4865\nEpoch [8/20], Batch [2000/5108], Loss: 5066.9917\nEpoch [8/20], Batch [2100/5108], Loss: 2557.6538\nEpoch [8/20], Batch [2200/5108], Loss: 1995.8990\nEpoch [8/20], Batch [2300/5108], Loss: 3012.9890\nEpoch [8/20], Batch [2400/5108], Loss: 1843.4220\nEpoch [8/20], Batch [2500/5108], Loss: 3032.8999\nEpoch [8/20], Batch [2600/5108], Loss: 2418.4690\nEpoch [8/20], Batch [2700/5108], Loss: 2570.4414\nEpoch [8/20], Batch [2800/5108], Loss: 2467.2610\nEpoch [8/20], Batch [2900/5108], Loss: 1949.2076\nEpoch [8/20], Batch [3000/5108], Loss: 2890.8499\nEpoch [8/20], Batch [3100/5108], Loss: 1493.7777\nEpoch [8/20], Batch [3200/5108], Loss: 2193.8633\nEpoch [8/20], Batch [3300/5108], Loss: 2053.9536\nEpoch [8/20], Batch [3400/5108], Loss: 1625.8231\nEpoch [8/20], Batch [3500/5108], Loss: 2553.6902\nEpoch [8/20], Batch [3600/5108], Loss: 2491.2112\nEpoch [8/20], Batch [3700/5108], Loss: 1467.2461\nEpoch [8/20], Batch [3800/5108], Loss: 2208.0334\nEpoch [8/20], Batch [3900/5108], Loss: 1973.6466\nEpoch [8/20], Batch [4000/5108], Loss: 2349.3625\nEpoch [8/20], Batch [4100/5108], Loss: 1800.7094\nEpoch [8/20], Batch [4200/5108], Loss: 3247.6577\nEpoch [8/20], Batch [4300/5108], Loss: 1784.4088\nEpoch [8/20], Batch [4400/5108], Loss: 2665.1106\nEpoch [8/20], Batch [4500/5108], Loss: 2306.1133\nEpoch [8/20], Batch [4600/5108], Loss: 1779.3978\nEpoch [8/20], Batch [4700/5108], Loss: 2402.0254\nEpoch [8/20], Batch [4800/5108], Loss: 3571.3276\nEpoch [8/20], Batch [4900/5108], Loss: 1782.1501\nEpoch [8/20], Batch [5000/5108], Loss: 1710.9957\nEpoch [8/20], Batch [5100/5108], Loss: 2101.8477\nEpoch [8/20], Average Loss: 2318.9369\nEpoch [9/20], Batch [100/5108], Loss: 2269.0571\nEpoch [9/20], Batch [200/5108], Loss: 2801.5447\nEpoch [9/20], Batch [300/5108], Loss: 1894.6047\nEpoch [9/20], Batch [400/5108], Loss: 2010.3969\nEpoch [9/20], Batch [500/5108], Loss: 2002.5586\nEpoch [9/20], Batch [600/5108], Loss: 2205.6531\nEpoch [9/20], Batch [700/5108], Loss: 1963.0826\nEpoch [9/20], Batch [800/5108], Loss: 2235.6338\nEpoch [9/20], Batch [900/5108], Loss: 1960.9122\nEpoch [9/20], Batch [1000/5108], Loss: 4095.1309\nEpoch [9/20], Batch [1100/5108], Loss: 2422.3098\nEpoch [9/20], Batch [1200/5108], Loss: 2355.2444\nEpoch [9/20], Batch [1300/5108], Loss: 2169.4746\nEpoch [9/20], Batch [1400/5108], Loss: 2391.4048\nEpoch [9/20], Batch [1500/5108], Loss: 1956.9092\nEpoch [9/20], Batch [1600/5108], Loss: 4526.2720\nEpoch [9/20], Batch [1700/5108], Loss: 1982.4535\nEpoch [9/20], Batch [1800/5108], Loss: 2977.0364\nEpoch [9/20], Batch [1900/5108], Loss: 1953.0430\nEpoch [9/20], Batch [2000/5108], Loss: 2705.7078\nEpoch [9/20], Batch [2100/5108], Loss: 2414.4827\nEpoch [9/20], Batch [2200/5108], Loss: 2492.8743\nEpoch [9/20], Batch [2300/5108], Loss: 1798.8558\nEpoch [9/20], Batch [2400/5108], Loss: 2212.8882\nEpoch [9/20], Batch [2500/5108], Loss: 2305.2192\nEpoch [9/20], Batch [2600/5108], Loss: 2587.2288\nEpoch [9/20], Batch [2700/5108], Loss: 2256.4338\nEpoch [9/20], Batch [2800/5108], Loss: 2076.0977\nEpoch [9/20], Batch [2900/5108], Loss: 1964.1503\nEpoch [9/20], Batch [3000/5108], Loss: 2921.5276\nEpoch [9/20], Batch [3100/5108], Loss: 2274.5911\nEpoch [9/20], Batch [3200/5108], Loss: 2234.6174\nEpoch [9/20], Batch [3300/5108], Loss: 1490.7336\nEpoch [9/20], Batch [3400/5108], Loss: 2234.9443\nEpoch [9/20], Batch [3500/5108], Loss: 2708.1609\nEpoch [9/20], Batch [3600/5108], Loss: 1353.7073\nEpoch [9/20], Batch [3700/5108], Loss: 3207.0503\nEpoch [9/20], Batch [3800/5108], Loss: 2348.2590\nEpoch [9/20], Batch [3900/5108], Loss: 2174.2102\nEpoch [9/20], Batch [4000/5108], Loss: 1713.2061\nEpoch [9/20], Batch [4100/5108], Loss: 3367.1626\nEpoch [9/20], Batch [4200/5108], Loss: 2603.0898\nEpoch [9/20], Batch [4300/5108], Loss: 1840.3641\nEpoch [9/20], Batch [4400/5108], Loss: 2157.1667\nEpoch [9/20], Batch [4500/5108], Loss: 2432.1440\nEpoch [9/20], Batch [4600/5108], Loss: 3539.1660\nEpoch [9/20], Batch [4700/5108], Loss: 2747.7942\nEpoch [9/20], Batch [4800/5108], Loss: 2449.4341\nEpoch [9/20], Batch [4900/5108], Loss: 1897.5020\nEpoch [9/20], Batch [5000/5108], Loss: 2000.8451\nEpoch [9/20], Batch [5100/5108], Loss: 2118.7031\nEpoch [9/20], Average Loss: 2277.3834\nEpoch [10/20], Batch [100/5108], Loss: 1896.4323\nEpoch [10/20], Batch [200/5108], Loss: 1869.2059\nEpoch [10/20], Batch [300/5108], Loss: 1902.0988\nEpoch [10/20], Batch [400/5108], Loss: 1995.2272\nEpoch [10/20], Batch [500/5108], Loss: 2497.0842\nEpoch [10/20], Batch [600/5108], Loss: 2446.8503\nEpoch [10/20], Batch [700/5108], Loss: 2484.5769\nEpoch [10/20], Batch [800/5108], Loss: 1989.4648\nEpoch [10/20], Batch [900/5108], Loss: 1980.5192\nEpoch [10/20], Batch [1000/5108], Loss: 2178.9722\nEpoch [10/20], Batch [1100/5108], Loss: 2066.9817\nEpoch [10/20], Batch [1200/5108], Loss: 1768.6387\nEpoch [10/20], Batch [1300/5108], Loss: 2211.4280\nEpoch [10/20], Batch [1400/5108], Loss: 2249.8398\nEpoch [10/20], Batch [1500/5108], Loss: 1630.3751\nEpoch [10/20], Batch [1600/5108], Loss: 2490.0410\nEpoch [10/20], Batch [1700/5108], Loss: 2356.8789\nEpoch [10/20], Batch [1800/5108], Loss: 2368.7249\nEpoch [10/20], Batch [1900/5108], Loss: 2100.7031\nEpoch [10/20], Batch [2000/5108], Loss: 2532.1589\nEpoch [10/20], Batch [2100/5108], Loss: 1773.4000\nEpoch [10/20], Batch [2200/5108], Loss: 3323.4114\nEpoch [10/20], Batch [2300/5108], Loss: 2221.4749\nEpoch [10/20], Batch [2400/5108], Loss: 2075.5920\nEpoch [10/20], Batch [2500/5108], Loss: 1528.6398\nEpoch [10/20], Batch [2600/5108], Loss: 2332.5039\nEpoch [10/20], Batch [2700/5108], Loss: 2393.5002\nEpoch [10/20], Batch [2800/5108], Loss: 1216.7018\nEpoch [10/20], Batch [2900/5108], Loss: 2263.8950\nEpoch [10/20], Batch [3000/5108], Loss: 1591.5070\nEpoch [10/20], Batch [3100/5108], Loss: 1826.8582\nEpoch [10/20], Batch [3200/5108], Loss: 2897.2349\nEpoch [10/20], Batch [3300/5108], Loss: 2094.5059\nEpoch [10/20], Batch [3400/5108], Loss: 1391.7559\nEpoch [10/20], Batch [3500/5108], Loss: 1763.0126\nEpoch [10/20], Batch [3600/5108], Loss: 2206.2778\nEpoch [10/20], Batch [3700/5108], Loss: 1959.5625\nEpoch [10/20], Batch [3800/5108], Loss: 2866.8264\nEpoch [10/20], Batch [3900/5108], Loss: 2088.3855\nEpoch [10/20], Batch [4000/5108], Loss: 5025.3828\nEpoch [10/20], Batch [4100/5108], Loss: 2114.2322\nEpoch [10/20], Batch [4200/5108], Loss: 1804.2548\nEpoch [10/20], Batch [4300/5108], Loss: 1922.7670\nEpoch [10/20], Batch [4400/5108], Loss: 1966.2267\nEpoch [10/20], Batch [4500/5108], Loss: 2402.7405\nEpoch [10/20], Batch [4600/5108], Loss: 1870.5554\nEpoch [10/20], Batch [4700/5108], Loss: 2006.3605\nEpoch [10/20], Batch [4800/5108], Loss: 1525.2888\nEpoch [10/20], Batch [4900/5108], Loss: 1606.7225\nEpoch [10/20], Batch [5000/5108], Loss: 2347.8032\nEpoch [10/20], Batch [5100/5108], Loss: 1861.6755\nEpoch [10/20], Average Loss: 2251.6905\nEpoch [11/20], Batch [100/5108], Loss: 2274.0044\nEpoch [11/20], Batch [200/5108], Loss: 1636.8086\nEpoch [11/20], Batch [300/5108], Loss: 1019.9639\nEpoch [11/20], Batch [400/5108], Loss: 2184.2192\nEpoch [11/20], Batch [500/5108], Loss: 2578.1230\nEpoch [11/20], Batch [600/5108], Loss: 1386.4301\nEpoch [11/20], Batch [700/5108], Loss: 1900.0183\nEpoch [11/20], Batch [800/5108], Loss: 2173.8789\nEpoch [11/20], Batch [900/5108], Loss: 2431.5427\nEpoch [11/20], Batch [1000/5108], Loss: 1853.4347\nEpoch [11/20], Batch [1100/5108], Loss: 1945.8583\nEpoch [11/20], Batch [1200/5108], Loss: 2084.3538\nEpoch [11/20], Batch [1300/5108], Loss: 1914.7278\nEpoch [11/20], Batch [1400/5108], Loss: 1811.6766\nEpoch [11/20], Batch [1500/5108], Loss: 1817.0281\nEpoch [11/20], Batch [1600/5108], Loss: 3354.5288\nEpoch [11/20], Batch [1700/5108], Loss: 1810.7898\nEpoch [11/20], Batch [1800/5108], Loss: 3825.1211\nEpoch [11/20], Batch [1900/5108], Loss: 2004.6628\nEpoch [11/20], Batch [2000/5108], Loss: 2105.3088\nEpoch [11/20], Batch [2100/5108], Loss: 2562.1394\nEpoch [11/20], Batch [2200/5108], Loss: 3858.2500\nEpoch [11/20], Batch [2300/5108], Loss: 2112.5447\nEpoch [11/20], Batch [2400/5108], Loss: 1565.5579\nEpoch [11/20], Batch [2500/5108], Loss: 2025.4777\nEpoch [11/20], Batch [2600/5108], Loss: 2288.9539\nEpoch [11/20], Batch [2700/5108], Loss: 1659.2982\nEpoch [11/20], Batch [2800/5108], Loss: 2415.6582\nEpoch [11/20], Batch [2900/5108], Loss: 2845.5652\nEpoch [11/20], Batch [3000/5108], Loss: 1630.2570\nEpoch [11/20], Batch [3100/5108], Loss: 1937.9852\nEpoch [11/20], Batch [3200/5108], Loss: 1832.1650\nEpoch [11/20], Batch [3300/5108], Loss: 2458.9219\nEpoch [11/20], Batch [3400/5108], Loss: 3685.8347\nEpoch [11/20], Batch [3500/5108], Loss: 2573.6667\nEpoch [11/20], Batch [3600/5108], Loss: 3268.3811\nEpoch [11/20], Batch [3700/5108], Loss: 2128.9915\nEpoch [11/20], Batch [3800/5108], Loss: 1647.0352\nEpoch [11/20], Batch [3900/5108], Loss: 2889.6519\nEpoch [11/20], Batch [4000/5108], Loss: 2640.3298\nEpoch [11/20], Batch [4100/5108], Loss: 2730.0891\nEpoch [11/20], Batch [4200/5108], Loss: 2131.9788\nEpoch [11/20], Batch [4300/5108], Loss: 2299.5525\nEpoch [11/20], Batch [4400/5108], Loss: 1567.8757\nEpoch [11/20], Batch [4500/5108], Loss: 1833.6946\nEpoch [11/20], Batch [4600/5108], Loss: 1753.2825\nEpoch [11/20], Batch [4700/5108], Loss: 3321.1941\nEpoch [11/20], Batch [4800/5108], Loss: 2105.9875\nEpoch [11/20], Batch [4900/5108], Loss: 1616.3303\nEpoch [11/20], Batch [5000/5108], Loss: 1843.9138\nEpoch [11/20], Batch [5100/5108], Loss: 2122.7888\nEpoch [11/20], Average Loss: 2225.6423\nEpoch [12/20], Batch [100/5108], Loss: 4482.1914\nEpoch [12/20], Batch [200/5108], Loss: 2505.2114\nEpoch [12/20], Batch [300/5108], Loss: 1954.2916\nEpoch [12/20], Batch [400/5108], Loss: 2022.1805\nEpoch [12/20], Batch [500/5108], Loss: 2145.3330\nEpoch [12/20], Batch [600/5108], Loss: 2610.2668\nEpoch [12/20], Batch [700/5108], Loss: 1566.2330\nEpoch [12/20], Batch [800/5108], Loss: 1780.2073\nEpoch [12/20], Batch [900/5108], Loss: 2077.6711\nEpoch [12/20], Batch [1000/5108], Loss: 1549.0341\nEpoch [12/20], Batch [1100/5108], Loss: 2508.2117\nEpoch [12/20], Batch [1200/5108], Loss: 2073.8704\nEpoch [12/20], Batch [1300/5108], Loss: 2336.6555\nEpoch [12/20], Batch [1400/5108], Loss: 1793.3801\nEpoch [12/20], Batch [1500/5108], Loss: 1887.3180\nEpoch [12/20], Batch [1600/5108], Loss: 2053.1794\nEpoch [12/20], Batch [1700/5108], Loss: 4329.8252\nEpoch [12/20], Batch [1800/5108], Loss: 4509.3350\nEpoch [12/20], Batch [1900/5108], Loss: 1988.3422\nEpoch [12/20], Batch [2000/5108], Loss: 1944.5198\nEpoch [12/20], Batch [2100/5108], Loss: 1770.4661\nEpoch [12/20], Batch [2200/5108], Loss: 2924.9612\nEpoch [12/20], Batch [2300/5108], Loss: 1310.5222\nEpoch [12/20], Batch [2400/5108], Loss: 2001.7284\nEpoch [12/20], Batch [2500/5108], Loss: 1558.4675\nEpoch [12/20], Batch [2600/5108], Loss: 1426.2631\nEpoch [12/20], Batch [2700/5108], Loss: 1568.1506\nEpoch [12/20], Batch [2800/5108], Loss: 2102.7947\nEpoch [12/20], Batch [2900/5108], Loss: 2195.4077\nEpoch [12/20], Batch [3000/5108], Loss: 1271.6456\nEpoch [12/20], Batch [3100/5108], Loss: 2577.4338\nEpoch [12/20], Batch [3200/5108], Loss: 3202.7056\nEpoch [12/20], Batch [3300/5108], Loss: 2396.7532\nEpoch [12/20], Batch [3400/5108], Loss: 2455.7249\nEpoch [12/20], Batch [3500/5108], Loss: 1473.7880\nEpoch [12/20], Batch [3600/5108], Loss: 1691.5485\nEpoch [12/20], Batch [3700/5108], Loss: 1613.7965\nEpoch [12/20], Batch [3800/5108], Loss: 2691.1367\nEpoch [12/20], Batch [3900/5108], Loss: 1681.9700\nEpoch [12/20], Batch [4000/5108], Loss: 2215.8757\nEpoch [12/20], Batch [4100/5108], Loss: 2224.2083\nEpoch [12/20], Batch [4200/5108], Loss: 1837.4896\nEpoch [12/20], Batch [4300/5108], Loss: 2697.6077\nEpoch [12/20], Batch [4400/5108], Loss: 1780.7682\nEpoch [12/20], Batch [4500/5108], Loss: 1946.7079\nEpoch [12/20], Batch [4600/5108], Loss: 2221.4900\nEpoch [12/20], Batch [4700/5108], Loss: 2467.9258\nEpoch [12/20], Batch [4800/5108], Loss: 1539.6458\nEpoch [12/20], Batch [4900/5108], Loss: 2308.6309\nEpoch [12/20], Batch [5000/5108], Loss: 2331.6912\nEpoch [12/20], Batch [5100/5108], Loss: 1779.3817\nEpoch [12/20], Average Loss: 2190.1186\nEpoch [13/20], Batch [100/5108], Loss: 1879.5361\nEpoch [13/20], Batch [200/5108], Loss: 2028.2166\nEpoch [13/20], Batch [300/5108], Loss: 1843.9445\nEpoch [13/20], Batch [400/5108], Loss: 1714.1094\nEpoch [13/20], Batch [500/5108], Loss: 1585.0271\nEpoch [13/20], Batch [600/5108], Loss: 1870.0275\nEpoch [13/20], Batch [700/5108], Loss: 2114.5796\nEpoch [13/20], Batch [800/5108], Loss: 1878.4485\nEpoch [13/20], Batch [900/5108], Loss: 2510.0093\nEpoch [13/20], Batch [1000/5108], Loss: 1813.0089\nEpoch [13/20], Batch [1100/5108], Loss: 4786.7935\nEpoch [13/20], Batch [1200/5108], Loss: 1690.5225\nEpoch [13/20], Batch [1300/5108], Loss: 1938.7986\nEpoch [13/20], Batch [1400/5108], Loss: 2125.7783\nEpoch [13/20], Batch [1500/5108], Loss: 2217.8987\nEpoch [13/20], Batch [1600/5108], Loss: 2158.6914\nEpoch [13/20], Batch [1700/5108], Loss: 2125.3989\nEpoch [13/20], Batch [1800/5108], Loss: 2053.3809\nEpoch [13/20], Batch [1900/5108], Loss: 1587.8704\nEpoch [13/20], Batch [2000/5108], Loss: 2402.8879\nEpoch [13/20], Batch [2100/5108], Loss: 1876.5272\nEpoch [13/20], Batch [2200/5108], Loss: 1803.4695\nEpoch [13/20], Batch [2300/5108], Loss: 1886.7871\nEpoch [13/20], Batch [2400/5108], Loss: 2447.3816\nEpoch [13/20], Batch [2500/5108], Loss: 1744.9055\nEpoch [13/20], Batch [2600/5108], Loss: 1783.3773\nEpoch [13/20], Batch [2700/5108], Loss: 2413.8394\nEpoch [13/20], Batch [2800/5108], Loss: 2854.2590\nEpoch [13/20], Batch [2900/5108], Loss: 2497.5945\nEpoch [13/20], Batch [3000/5108], Loss: 1792.9475\nEpoch [13/20], Batch [3100/5108], Loss: 2551.3833\nEpoch [13/20], Batch [3200/5108], Loss: 1654.5812\nEpoch [13/20], Batch [3300/5108], Loss: 2002.9648\nEpoch [13/20], Batch [3400/5108], Loss: 1885.0045\nEpoch [13/20], Batch [3500/5108], Loss: 1964.3826\nEpoch [13/20], Batch [3600/5108], Loss: 2194.9080\nEpoch [13/20], Batch [3700/5108], Loss: 1981.2480\nEpoch [13/20], Batch [3800/5108], Loss: 1976.5360\nEpoch [13/20], Batch [3900/5108], Loss: 1591.3564\nEpoch [13/20], Batch [4000/5108], Loss: 1709.6996\nEpoch [13/20], Batch [4100/5108], Loss: 1665.6267\nEpoch [13/20], Batch [4200/5108], Loss: 2232.8284\nEpoch [13/20], Batch [4300/5108], Loss: 3098.7212\nEpoch [13/20], Batch [4400/5108], Loss: 2088.0425\nEpoch [13/20], Batch [4500/5108], Loss: 1949.2482\nEpoch [13/20], Batch [4600/5108], Loss: 2025.4481\nEpoch [13/20], Batch [4700/5108], Loss: 1815.7765\nEpoch [13/20], Batch [4800/5108], Loss: 1830.8572\nEpoch [13/20], Batch [4900/5108], Loss: 3686.0259\nEpoch [13/20], Batch [5000/5108], Loss: 2767.9724\nEpoch [13/20], Batch [5100/5108], Loss: 2880.5378\nEpoch [13/20], Average Loss: 2174.9579\nEpoch [14/20], Batch [100/5108], Loss: 2308.5422\nEpoch [14/20], Batch [200/5108], Loss: 3181.9761\nEpoch [14/20], Batch [300/5108], Loss: 2039.4160\nEpoch [14/20], Batch [400/5108], Loss: 1626.9885\nEpoch [14/20], Batch [500/5108], Loss: 1647.2754\nEpoch [14/20], Batch [600/5108], Loss: 3108.8962\nEpoch [14/20], Batch [700/5108], Loss: 1490.0148\nEpoch [14/20], Batch [800/5108], Loss: 2274.5459\nEpoch [14/20], Batch [900/5108], Loss: 1305.6346\nEpoch [14/20], Batch [1000/5108], Loss: 2305.7292\nEpoch [14/20], Batch [1100/5108], Loss: 2003.5282\nEpoch [14/20], Batch [1200/5108], Loss: 1862.8820\nEpoch [14/20], Batch [1300/5108], Loss: 2066.3499\nEpoch [14/20], Batch [1400/5108], Loss: 1690.7548\nEpoch [14/20], Batch [1500/5108], Loss: 1689.7728\nEpoch [14/20], Batch [1600/5108], Loss: 2180.3635\nEpoch [14/20], Batch [1700/5108], Loss: 3529.1250\nEpoch [14/20], Batch [1800/5108], Loss: 2318.7290\nEpoch [14/20], Batch [1900/5108], Loss: 1559.4525\nEpoch [14/20], Batch [2000/5108], Loss: 1669.8126\nEpoch [14/20], Batch [2100/5108], Loss: 1545.5748\nEpoch [14/20], Batch [2200/5108], Loss: 2038.9750\nEpoch [14/20], Batch [2300/5108], Loss: 1670.1293\nEpoch [14/20], Batch [2400/5108], Loss: 2215.2891\nEpoch [14/20], Batch [2500/5108], Loss: 2371.5955\nEpoch [14/20], Batch [2600/5108], Loss: 1897.1661\nEpoch [14/20], Batch [2700/5108], Loss: 2118.7849\nEpoch [14/20], Batch [2800/5108], Loss: 2578.4827\nEpoch [14/20], Batch [2900/5108], Loss: 1761.2194\nEpoch [14/20], Batch [3000/5108], Loss: 2214.8477\nEpoch [14/20], Batch [3100/5108], Loss: 3050.4236\nEpoch [14/20], Batch [3200/5108], Loss: 2834.0042\nEpoch [14/20], Batch [3300/5108], Loss: 2641.6831\nEpoch [14/20], Batch [3400/5108], Loss: 2322.1584\nEpoch [14/20], Batch [3500/5108], Loss: 2753.4626\nEpoch [14/20], Batch [3600/5108], Loss: 2178.4458\nEpoch [14/20], Batch [3700/5108], Loss: 3115.7659\nEpoch [14/20], Batch [3800/5108], Loss: 2001.5917\nEpoch [14/20], Batch [3900/5108], Loss: 1810.7284\nEpoch [14/20], Batch [4000/5108], Loss: 3047.5730\nEpoch [14/20], Batch [4100/5108], Loss: 1338.9348\nEpoch [14/20], Batch [4200/5108], Loss: 2190.1318\nEpoch [14/20], Batch [4300/5108], Loss: 3382.4985\nEpoch [14/20], Batch [4400/5108], Loss: 1949.9807\nEpoch [14/20], Batch [4500/5108], Loss: 1995.4153\nEpoch [14/20], Batch [4600/5108], Loss: 1388.6005\nEpoch [14/20], Batch [4700/5108], Loss: 1736.8634\nEpoch [14/20], Batch [4800/5108], Loss: 1778.8895\nEpoch [14/20], Batch [4900/5108], Loss: 1738.0592\nEpoch [14/20], Batch [5000/5108], Loss: 1889.4388\nEpoch [14/20], Batch [5100/5108], Loss: 2342.6851\nEpoch [14/20], Average Loss: 2158.3218\nEpoch [15/20], Batch [100/5108], Loss: 2429.4487\nEpoch [15/20], Batch [200/5108], Loss: 2970.0376\nEpoch [15/20], Batch [300/5108], Loss: 1901.4872\nEpoch [15/20], Batch [400/5108], Loss: 2492.3245\nEpoch [15/20], Batch [500/5108], Loss: 2005.8451\nEpoch [15/20], Batch [600/5108], Loss: 1762.7679\nEpoch [15/20], Batch [700/5108], Loss: 2199.2148\nEpoch [15/20], Batch [800/5108], Loss: 1963.7474\nEpoch [15/20], Batch [900/5108], Loss: 1892.3080\nEpoch [15/20], Batch [1000/5108], Loss: 2274.9929\nEpoch [15/20], Batch [1100/5108], Loss: 1382.8375\nEpoch [15/20], Batch [1200/5108], Loss: 2387.3003\nEpoch [15/20], Batch [1300/5108], Loss: 1718.8594\nEpoch [15/20], Batch [1400/5108], Loss: 1582.5414\nEpoch [15/20], Batch [1500/5108], Loss: 2169.0540\nEpoch [15/20], Batch [1600/5108], Loss: 2456.2800\nEpoch [15/20], Batch [1700/5108], Loss: 1517.5470\nEpoch [15/20], Batch [1800/5108], Loss: 1829.1158\nEpoch [15/20], Batch [1900/5108], Loss: 2327.9775\nEpoch [15/20], Batch [2000/5108], Loss: 1682.9973\nEpoch [15/20], Batch [2100/5108], Loss: 3438.5403\nEpoch [15/20], Batch [2200/5108], Loss: 2237.2065\nEpoch [15/20], Batch [2300/5108], Loss: 1620.6414\nEpoch [15/20], Batch [2400/5108], Loss: 2924.3574\nEpoch [15/20], Batch [2500/5108], Loss: 2017.5455\nEpoch [15/20], Batch [2600/5108], Loss: 2593.6113\nEpoch [15/20], Batch [2700/5108], Loss: 1927.3094\nEpoch [15/20], Batch [2800/5108], Loss: 2212.6116\nEpoch [15/20], Batch [2900/5108], Loss: 2393.9065\nEpoch [15/20], Batch [3000/5108], Loss: 2649.6926\nEpoch [15/20], Batch [3100/5108], Loss: 2127.5952\nEpoch [15/20], Batch [3200/5108], Loss: 1959.7637\nEpoch [15/20], Batch [3300/5108], Loss: 1381.3934\nEpoch [15/20], Batch [3400/5108], Loss: 3692.7070\nEpoch [15/20], Batch [3500/5108], Loss: 1641.5769\nEpoch [15/20], Batch [3600/5108], Loss: 1778.7889\nEpoch [15/20], Batch [3700/5108], Loss: 3380.7161\nEpoch [15/20], Batch [3800/5108], Loss: 2421.4194\nEpoch [15/20], Batch [3900/5108], Loss: 2043.6702\nEpoch [15/20], Batch [4000/5108], Loss: 1371.8016\nEpoch [15/20], Batch [4100/5108], Loss: 2175.9363\nEpoch [15/20], Batch [4200/5108], Loss: 1754.6398\nEpoch [15/20], Batch [4300/5108], Loss: 1773.3254\nEpoch [15/20], Batch [4400/5108], Loss: 1727.3523\nEpoch [15/20], Batch [4500/5108], Loss: 2345.4841\nEpoch [15/20], Batch [4600/5108], Loss: 1857.7958\nEpoch [15/20], Batch [4700/5108], Loss: 2860.5300\nEpoch [15/20], Batch [4800/5108], Loss: 1527.0735\nEpoch [15/20], Batch [4900/5108], Loss: 2536.7554\nEpoch [15/20], Batch [5000/5108], Loss: 1629.1730\nEpoch [15/20], Batch [5100/5108], Loss: 2532.8706\nEpoch [15/20], Average Loss: 2166.4032\nEpoch [16/20], Batch [100/5108], Loss: 2725.7825\nEpoch [16/20], Batch [200/5108], Loss: 2493.5957\nEpoch [16/20], Batch [300/5108], Loss: 2173.2725\nEpoch [16/20], Batch [400/5108], Loss: 1711.9288\nEpoch [16/20], Batch [500/5108], Loss: 2297.4878\nEpoch [16/20], Batch [600/5108], Loss: 1578.3151\nEpoch [16/20], Batch [700/5108], Loss: 1738.7068\nEpoch [16/20], Batch [800/5108], Loss: 2211.9424\nEpoch [16/20], Batch [900/5108], Loss: 1831.9929\nEpoch [16/20], Batch [1000/5108], Loss: 1661.5890\nEpoch [16/20], Batch [1100/5108], Loss: 2571.3108\nEpoch [16/20], Batch [1200/5108], Loss: 1794.0592\nEpoch [16/20], Batch [1300/5108], Loss: 1510.5480\nEpoch [16/20], Batch [1400/5108], Loss: 2191.4609\nEpoch [16/20], Batch [1500/5108], Loss: 2257.8450\nEpoch [16/20], Batch [1600/5108], Loss: 2694.2385\nEpoch [16/20], Batch [1700/5108], Loss: 2146.1565\nEpoch [16/20], Batch [1800/5108], Loss: 1660.2047\nEpoch [16/20], Batch [1900/5108], Loss: 2501.8992\nEpoch [16/20], Batch [2000/5108], Loss: 1971.5847\nEpoch [16/20], Batch [2100/5108], Loss: 1535.0940\nEpoch [16/20], Batch [2200/5108], Loss: 1497.9530\nEpoch [16/20], Batch [2300/5108], Loss: 2932.8784\nEpoch [16/20], Batch [2400/5108], Loss: 1726.4432\nEpoch [16/20], Batch [2500/5108], Loss: 2041.0227\nEpoch [16/20], Batch [2600/5108], Loss: 1889.8588\nEpoch [16/20], Batch [2700/5108], Loss: 1627.2997\nEpoch [16/20], Batch [2800/5108], Loss: 1564.5101\nEpoch [16/20], Batch [2900/5108], Loss: 1839.0935\nEpoch [16/20], Batch [3000/5108], Loss: 1973.8330\nEpoch [16/20], Batch [3100/5108], Loss: 1515.8320\nEpoch [16/20], Batch [3200/5108], Loss: 2161.1526\nEpoch [16/20], Batch [3300/5108], Loss: 1655.1201\nEpoch [16/20], Batch [3400/5108], Loss: 2625.9329\nEpoch [16/20], Batch [3500/5108], Loss: 1552.7405\nEpoch [16/20], Batch [3600/5108], Loss: 1836.6415\nEpoch [16/20], Batch [3700/5108], Loss: 2192.3850\nEpoch [16/20], Batch [3800/5108], Loss: 1643.2244\nEpoch [16/20], Batch [3900/5108], Loss: 1896.0321\nEpoch [16/20], Batch [4000/5108], Loss: 1673.5109\nEpoch [16/20], Batch [4100/5108], Loss: 2941.2817\nEpoch [16/20], Batch [4200/5108], Loss: 2247.6228\nEpoch [16/20], Batch [4300/5108], Loss: 2206.5308\nEpoch [16/20], Batch [4400/5108], Loss: 1437.3759\nEpoch [16/20], Batch [4500/5108], Loss: 3766.1428\nEpoch [16/20], Batch [4600/5108], Loss: 1864.8143\nEpoch [16/20], Batch [4700/5108], Loss: 1972.3562\nEpoch [16/20], Batch [4800/5108], Loss: 1807.7526\nEpoch [16/20], Batch [4900/5108], Loss: 2368.5789\nEpoch [16/20], Batch [5000/5108], Loss: 1907.8051\nEpoch [16/20], Batch [5100/5108], Loss: 1521.8662\nEpoch [16/20], Average Loss: 2112.6363\nEpoch [17/20], Batch [100/5108], Loss: 1446.6146\nEpoch [17/20], Batch [200/5108], Loss: 2735.3960\nEpoch [17/20], Batch [300/5108], Loss: 1598.9785\nEpoch [17/20], Batch [400/5108], Loss: 2003.0266\nEpoch [17/20], Batch [500/5108], Loss: 1733.2834\nEpoch [17/20], Batch [600/5108], Loss: 1953.3182\nEpoch [17/20], Batch [700/5108], Loss: 1631.2274\nEpoch [17/20], Batch [800/5108], Loss: 2089.6802\nEpoch [17/20], Batch [900/5108], Loss: 2194.1331\nEpoch [17/20], Batch [1000/5108], Loss: 2067.1365\nEpoch [17/20], Batch [1100/5108], Loss: 1920.8406\nEpoch [17/20], Batch [1200/5108], Loss: 1680.1893\nEpoch [17/20], Batch [1300/5108], Loss: 2185.2485\nEpoch [17/20], Batch [1400/5108], Loss: 2213.4771\nEpoch [17/20], Batch [1500/5108], Loss: 1642.2133\nEpoch [17/20], Batch [1600/5108], Loss: 2567.3062\nEpoch [17/20], Batch [1700/5108], Loss: 2060.3718\nEpoch [17/20], Batch [1800/5108], Loss: 1629.0831\nEpoch [17/20], Batch [1900/5108], Loss: 2400.6902\nEpoch [17/20], Batch [2000/5108], Loss: 1526.8066\nEpoch [17/20], Batch [2100/5108], Loss: 2119.2517\nEpoch [17/20], Batch [2200/5108], Loss: 2181.6125\nEpoch [17/20], Batch [2300/5108], Loss: 2245.2180\nEpoch [17/20], Batch [2400/5108], Loss: 1436.3719\nEpoch [17/20], Batch [2500/5108], Loss: 1908.5553\nEpoch [17/20], Batch [2600/5108], Loss: 2295.7419\nEpoch [17/20], Batch [2700/5108], Loss: 2258.4514\nEpoch [17/20], Batch [2800/5108], Loss: 1776.0720\nEpoch [17/20], Batch [2900/5108], Loss: 2079.9993\nEpoch [17/20], Batch [3000/5108], Loss: 2787.4456\nEpoch [17/20], Batch [3100/5108], Loss: 1861.9855\nEpoch [17/20], Batch [3200/5108], Loss: 2626.1174\nEpoch [17/20], Batch [3300/5108], Loss: 2860.2439\nEpoch [17/20], Batch [3400/5108], Loss: 2152.4277\nEpoch [17/20], Batch [3500/5108], Loss: 1553.5170\nEpoch [17/20], Batch [3600/5108], Loss: 1698.7592\nEpoch [17/20], Batch [3700/5108], Loss: 1852.5125\nEpoch [17/20], Batch [3800/5108], Loss: 1992.3754\nEpoch [17/20], Batch [3900/5108], Loss: 1703.0875\nEpoch [17/20], Batch [4000/5108], Loss: 1736.9753\nEpoch [17/20], Batch [4100/5108], Loss: 1943.6624\nEpoch [17/20], Batch [4200/5108], Loss: 2452.6128\nEpoch [17/20], Batch [4300/5108], Loss: 1613.4309\nEpoch [17/20], Batch [4400/5108], Loss: 2242.9756\nEpoch [17/20], Batch [4500/5108], Loss: 1255.8456\nEpoch [17/20], Batch [4600/5108], Loss: 1676.3783\nEpoch [17/20], Batch [4700/5108], Loss: 3530.8052\nEpoch [17/20], Batch [4800/5108], Loss: 1969.0839\nEpoch [17/20], Batch [4900/5108], Loss: 1804.0049\nEpoch [17/20], Batch [5000/5108], Loss: 1593.9843\nEpoch [17/20], Batch [5100/5108], Loss: 1655.1426\nEpoch [17/20], Average Loss: 2118.1530\nEpoch [18/20], Batch [100/5108], Loss: 2067.2932\nEpoch [18/20], Batch [200/5108], Loss: 2211.8660\nEpoch [18/20], Batch [300/5108], Loss: 2683.5432\nEpoch [18/20], Batch [400/5108], Loss: 3861.7415\nEpoch [18/20], Batch [500/5108], Loss: 3643.8594\nEpoch [18/20], Batch [600/5108], Loss: 1756.5201\nEpoch [18/20], Batch [700/5108], Loss: 1748.4768\nEpoch [18/20], Batch [800/5108], Loss: 1313.0599\nEpoch [18/20], Batch [900/5108], Loss: 1918.7106\nEpoch [18/20], Batch [1000/5108], Loss: 1793.7639\nEpoch [18/20], Batch [1100/5108], Loss: 1877.6522\nEpoch [18/20], Batch [1200/5108], Loss: 2305.1431\nEpoch [18/20], Batch [1300/5108], Loss: 2250.7659\nEpoch [18/20], Batch [1400/5108], Loss: 1982.5507\nEpoch [18/20], Batch [1500/5108], Loss: 1856.4781\nEpoch [18/20], Batch [1600/5108], Loss: 1383.3014\nEpoch [18/20], Batch [1700/5108], Loss: 1625.7203\nEpoch [18/20], Batch [1800/5108], Loss: 2800.9011\nEpoch [18/20], Batch [1900/5108], Loss: 2596.1953\nEpoch [18/20], Batch [2000/5108], Loss: 2097.4397\nEpoch [18/20], Batch [2100/5108], Loss: 1582.8402\nEpoch [18/20], Batch [2200/5108], Loss: 1965.5544\nEpoch [18/20], Batch [2300/5108], Loss: 2490.1191\nEpoch [18/20], Batch [2400/5108], Loss: 1706.6663\nEpoch [18/20], Batch [2500/5108], Loss: 1633.0031\nEpoch [18/20], Batch [2600/5108], Loss: 1735.2600\nEpoch [18/20], Batch [2700/5108], Loss: 1834.7838\nEpoch [18/20], Batch [2800/5108], Loss: 1927.5670\nEpoch [18/20], Batch [2900/5108], Loss: 1742.9369\nEpoch [18/20], Batch [3000/5108], Loss: 1391.5192\nEpoch [18/20], Batch [3100/5108], Loss: 1741.8536\nEpoch [18/20], Batch [3200/5108], Loss: 2060.5496\nEpoch [18/20], Batch [3300/5108], Loss: 1829.8928\nEpoch [18/20], Batch [3400/5108], Loss: 1961.7972\nEpoch [18/20], Batch [3500/5108], Loss: 2139.9644\nEpoch [18/20], Batch [3600/5108], Loss: 1713.7330\nEpoch [18/20], Batch [3700/5108], Loss: 1485.1556\nEpoch [18/20], Batch [3800/5108], Loss: 1601.1080\nEpoch [18/20], Batch [3900/5108], Loss: 1525.7258\nEpoch [18/20], Batch [4000/5108], Loss: 1350.8566\nEpoch [18/20], Batch [4100/5108], Loss: 2291.9270\nEpoch [18/20], Batch [4200/5108], Loss: 3306.4783\nEpoch [18/20], Batch [4300/5108], Loss: 2062.1182\nEpoch [18/20], Batch [4400/5108], Loss: 3071.6392\nEpoch [18/20], Batch [4500/5108], Loss: 1881.9745\nEpoch [18/20], Batch [4600/5108], Loss: 1919.7483\nEpoch [18/20], Batch [4700/5108], Loss: 2630.7468\nEpoch [18/20], Batch [4800/5108], Loss: 1494.9989\nEpoch [18/20], Batch [4900/5108], Loss: 1938.8273\nEpoch [18/20], Batch [5000/5108], Loss: 2180.1536\nEpoch [18/20], Batch [5100/5108], Loss: 1560.5946\nEpoch [18/20], Average Loss: 2093.1087\nEpoch [19/20], Batch [100/5108], Loss: 2366.4458\nEpoch [19/20], Batch [200/5108], Loss: 3697.3887\nEpoch [19/20], Batch [300/5108], Loss: 2000.7211\nEpoch [19/20], Batch [400/5108], Loss: 1830.2135\nEpoch [19/20], Batch [500/5108], Loss: 3685.0913\nEpoch [19/20], Batch [600/5108], Loss: 2713.7517\nEpoch [19/20], Batch [700/5108], Loss: 2263.4749\nEpoch [19/20], Batch [800/5108], Loss: 2254.8645\nEpoch [19/20], Batch [900/5108], Loss: 2195.7661\nEpoch [19/20], Batch [1000/5108], Loss: 1690.9460\nEpoch [19/20], Batch [1100/5108], Loss: 2729.4114\nEpoch [19/20], Batch [1200/5108], Loss: 1628.1155\nEpoch [19/20], Batch [1300/5108], Loss: 2688.2227\nEpoch [19/20], Batch [1400/5108], Loss: 2162.2844\nEpoch [19/20], Batch [1500/5108], Loss: 3569.9976\nEpoch [19/20], Batch [1600/5108], Loss: 2139.4934\nEpoch [19/20], Batch [1700/5108], Loss: 1913.8387\nEpoch [19/20], Batch [1800/5108], Loss: 1677.1501\nEpoch [19/20], Batch [1900/5108], Loss: 1404.6521\nEpoch [19/20], Batch [2000/5108], Loss: 1981.0187\nEpoch [19/20], Batch [2100/5108], Loss: 2161.9780\nEpoch [19/20], Batch [2200/5108], Loss: 2370.2634\nEpoch [19/20], Batch [2300/5108], Loss: 1773.8203\nEpoch [19/20], Batch [2400/5108], Loss: 1853.6687\nEpoch [19/20], Batch [2500/5108], Loss: 1739.3978\nEpoch [19/20], Batch [2600/5108], Loss: 1948.2429\nEpoch [19/20], Batch [2700/5108], Loss: 1691.2413\nEpoch [19/20], Batch [2800/5108], Loss: 2536.5916\nEpoch [19/20], Batch [2900/5108], Loss: 4022.5493\nEpoch [19/20], Batch [3000/5108], Loss: 1875.4928\nEpoch [19/20], Batch [3100/5108], Loss: 1413.3105\nEpoch [19/20], Batch [3200/5108], Loss: 1580.0612\nEpoch [19/20], Batch [3300/5108], Loss: 2130.0117\nEpoch [19/20], Batch [3400/5108], Loss: 1965.5848\nEpoch [19/20], Batch [3500/5108], Loss: 2335.3562\nEpoch [19/20], Batch [3600/5108], Loss: 2499.2258\nEpoch [19/20], Batch [3700/5108], Loss: 1474.0400\nEpoch [19/20], Batch [3800/5108], Loss: 2492.0603\nEpoch [19/20], Batch [3900/5108], Loss: 2345.5408\nEpoch [19/20], Batch [4000/5108], Loss: 1675.6683\nEpoch [19/20], Batch [4100/5108], Loss: 1614.9967\nEpoch [19/20], Batch [4200/5108], Loss: 2940.8323\nEpoch [19/20], Batch [4300/5108], Loss: 1708.2057\nEpoch [19/20], Batch [4400/5108], Loss: 1740.6708\nEpoch [19/20], Batch [4500/5108], Loss: 2119.4885\nEpoch [19/20], Batch [4600/5108], Loss: 2029.6649\nEpoch [19/20], Batch [4700/5108], Loss: 1678.0802\nEpoch [19/20], Batch [4800/5108], Loss: 2027.7610\nEpoch [19/20], Batch [4900/5108], Loss: 2035.1877\nEpoch [19/20], Batch [5000/5108], Loss: 2436.3921\nEpoch [19/20], Batch [5100/5108], Loss: 2131.0227\nEpoch [19/20], Average Loss: 2071.5577\nEpoch [20/20], Batch [100/5108], Loss: 1391.0475\nEpoch [20/20], Batch [200/5108], Loss: 1362.7871\nEpoch [20/20], Batch [300/5108], Loss: 1741.8239\nEpoch [20/20], Batch [400/5108], Loss: 1854.9570\nEpoch [20/20], Batch [500/5108], Loss: 1911.8591\nEpoch [20/20], Batch [600/5108], Loss: 2192.3728\nEpoch [20/20], Batch [700/5108], Loss: 1774.4926\nEpoch [20/20], Batch [800/5108], Loss: 2211.9041\nEpoch [20/20], Batch [900/5108], Loss: 1632.7190\nEpoch [20/20], Batch [1000/5108], Loss: 2236.3892\nEpoch [20/20], Batch [1100/5108], Loss: 1857.2305\nEpoch [20/20], Batch [1200/5108], Loss: 2136.2227\nEpoch [20/20], Batch [1300/5108], Loss: 1299.8701\nEpoch [20/20], Batch [1400/5108], Loss: 1525.4316\nEpoch [20/20], Batch [1500/5108], Loss: 2131.0847\nEpoch [20/20], Batch [1600/5108], Loss: 2004.7930\nEpoch [20/20], Batch [1700/5108], Loss: 1486.4498\nEpoch [20/20], Batch [1800/5108], Loss: 1556.8588\nEpoch [20/20], Batch [1900/5108], Loss: 1473.0212\nEpoch [20/20], Batch [2000/5108], Loss: 1211.0713\nEpoch [20/20], Batch [2100/5108], Loss: 2287.5212\nEpoch [20/20], Batch [2200/5108], Loss: 1610.8516\nEpoch [20/20], Batch [2300/5108], Loss: 2704.0315\nEpoch [20/20], Batch [2400/5108], Loss: 1580.0751\nEpoch [20/20], Batch [2500/5108], Loss: 1527.9434\nEpoch [20/20], Batch [2600/5108], Loss: 1758.3633\nEpoch [20/20], Batch [2700/5108], Loss: 2628.1763\nEpoch [20/20], Batch [2800/5108], Loss: 2138.1641\nEpoch [20/20], Batch [2900/5108], Loss: 2454.1987\nEpoch [20/20], Batch [3000/5108], Loss: 3019.0769\nEpoch [20/20], Batch [3100/5108], Loss: 2285.7817\nEpoch [20/20], Batch [3200/5108], Loss: 2539.3918\nEpoch [20/20], Batch [3300/5108], Loss: 1958.3873\nEpoch [20/20], Batch [3400/5108], Loss: 2086.1606\nEpoch [20/20], Batch [3500/5108], Loss: 1914.8690\nEpoch [20/20], Batch [3600/5108], Loss: 3478.7947\nEpoch [20/20], Batch [3700/5108], Loss: 1915.1898\nEpoch [20/20], Batch [3800/5108], Loss: 2130.3198\nEpoch [20/20], Batch [3900/5108], Loss: 2073.8596\nEpoch [20/20], Batch [4000/5108], Loss: 2040.1219\nEpoch [20/20], Batch [4100/5108], Loss: 2357.3254\nEpoch [20/20], Batch [4200/5108], Loss: 1656.4845\nEpoch [20/20], Batch [4300/5108], Loss: 2026.0017\nEpoch [20/20], Batch [4400/5108], Loss: 2075.1716\nEpoch [20/20], Batch [4500/5108], Loss: 2756.8687\nEpoch [20/20], Batch [4600/5108], Loss: 2126.5752\nEpoch [20/20], Batch [4700/5108], Loss: 1312.8613\nEpoch [20/20], Batch [4800/5108], Loss: 1405.4303\nEpoch [20/20], Batch [4900/5108], Loss: 2184.3481\nEpoch [20/20], Batch [5000/5108], Loss: 1674.7527\nEpoch [20/20], Batch [5100/5108], Loss: 3037.0178\nEpoch [20/20], Average Loss: 2043.8359\nFinal RMSE: 14.7383\n","output_type":"stream"}]},{"cell_type":"code","source":"import pandas as pd\n\n\ndf = df_new \n\n# Define the split ratio (e.g., 80% train, 20% test)\nsplit_ratio = 0.8\nsplit_index = int(len(df) * split_ratio)\n\n# Split the dataset\ntrain_df = df[:split_index]  # First 80% for training\ntest_df = df[split_index:]    # Remaining 20% for testing\n\n# Optionally, reset index for both DataFrames\ntrain_df.reset_index(drop=True, inplace=True)\ntest_df.reset_index(drop=True, inplace=True)\n\n# Check the shape of the splits\nprint(\"Training set shape:\", train_df.shape)\nprint(\"Testing set shape:\", test_df.shape)\n\n","metadata":{"execution":{"iopub.status.busy":"2024-10-30T10:47:20.564244Z","iopub.execute_input":"2024-10-30T10:47:20.564730Z","iopub.status.idle":"2024-10-30T10:47:20.575135Z","shell.execute_reply.started":"2024-10-30T10:47:20.564676Z","shell.execute_reply":"2024-10-30T10:47:20.573745Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stdout","text":"Training set shape: (326850, 50)\nTesting set shape: (81713, 50)\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}